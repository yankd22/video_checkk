## 闭源视频审核平台

### 1) Amazon Rekognition

* **标签体系**：官方采用**三级层级**，每个 L1 对应若干 L2，部分 L2 下有 L3。详见“Using the image and video moderation APIs”。该页还提供**完整标签清单的下载**入口。([AWS 文档][1])
* **自定义方式**：

  * **Custom Moderation**：上传并标注你的图像，训练“adapter”，调用 `DetectModerationLabels` 时带上 adapter 可提升在你域内的召回/精度。([AWS 文档][2])
  * **Custom Labels（独立服务）**：训练你自己的识别类目（分类/检测均可），与内容审核流程编排集成。([AWS 文档][12])

### 2) Hive AI

* **标签体系**：视觉审核文档页列出**性/暴力/毒品/仇恨/图像属性**等大类及**大量子类**，API 指南说明返回结构；Dashboard 可配置**阈值与动作规则**（无需自写后处理）。([docs.thehive.ai][3])
* **自定义方式**：

  * **Hive AutoML / Custom Model**：可为审核任务**新增类**或在现有头上增量数据微调；训练后模型通过 API 与 Dashboard 一致使用。([thehive.ai][4])
  * **策略层自定义**：在 Dashboard 里按类设置**阈值**与**规则**（封禁、限流、人工复核等）。([docs.thehive.ai][13])

### 3) Alibaba Cloud（内容安全/绿网）

* **标签/场景**：接口页与教程页列出**可选场景**（porn/terrorism/ad/undesirable/logo/audio-antispam 等），新版“视频审核2.0/增强版”同样按场景治理；IMM 的“Content moderation results”也给出多类结果标签参考。([阿里云][6], [阿里云帮助中心][7])
* **完整清单入口**：各场景枚举与返回值在**对应接口文档**中给出（例如异步检测与反馈接口的 `scenes` 取值）。([阿里云][6])
* **自定义方式**：

  * **自定义文本库/图片库**：控制台与 API 管理（如“Manage custom image libraries”、“DescribeImageLib”）；
  * 通过 **BizType** 绑定自定义库与策略，调用审核时指定场景+BizType 即可生效。([阿里云][8])

### 4) 腾讯云（视频内容安全 VM）

* **标签体系**：文档的**数据结构页**明确了返回标签：`Normal / Porn / Abuse / Ad / Custom`，并支持 `SubLabel`、`Suggestion` 等字段；视频/音频/图片/文本的文档均有相应数据结构说明。([腾讯云][9], [CloudCache][10])
* **完整清单入口**：以“**Data Types / 数据结构**”文档为准（国际站与中文站都有），其中对标签取值、子标签字段、置信度、定位框等有定义。([腾讯云][9])
* **自定义方式**：

  * **自定义词库/图库**：控制台“名单管理→关键词名单→自定义名单”创建并**在策略中关联**；命中后返回 `Label=Custom`，并带 `LibId/LibName` 字段以便你区分来源。([腾讯云][11], [CloudCache][10])

---

下面是对 **Hi‑Guard**（Hierarchical Guard）项目的全面梳理，以更清晰结构阐述它的方法、数据、标签分类、模型机制及开源情况：

---
# **Hi-Guard**

## 一、具体方法（Methodology）

**Hi-Guard** 是一个多模态内容审核框架，其设计特点包括：

1. **二阶段审核管道**：

   * **Stage 1 – Binary Guard**：使用轻量模型二分类判断安全内容并快速过滤。
   * **Stage 2 – Hierarchical Guard**：对疑似风险内容，依据**四层层级标签体系**执行路径分类（Domain → Topic → Subtype → Behavior），并输出结构化判定路径。([arXiv][1])

2. **策略对齐（Policy-Aligned Reasoning）**：

   * 平台审核规则以可读形式融入模型 Prompt，促使模型输出解释性推理链（Chain‑of‑Thought）和路径分类，提升透明度与可审计性。([arXiv][1])

3. **奖励机制与优化**：

   * 引入**多级软边界奖励（Soft‑Margin Reward）**，对混淆同层兄弟标签进行更高惩罚，以提升结构分类精细度。
   * 使用 **Group Relative Policy Optimization（GRPO）** 将奖励机制整合入训练，优化分类准确性与解释逻辑质量。([arXiv][1])

---

## 二、实验数据集（Datasets）

| Stage   | 任务            | 样本量    | 风险比例             |
| ------- | ------------- | ------ | ---------------- |
| Stage 1 | 二分类（安全 vs 风险） | 38,000 | Risky:Safe ≈ 1:4 |
| Stage 2 | 层级路径分类        | 41,000 | Risky:Safe ≈ 1:1 |

仅 Stage 1 判定风险的样本进入 Stage 2 进行“四层层级分类”。([GitHub][2])

---

## 三、标签分类（Hierarchical Taxonomy）

采用 **四层路径式分类体系**，如下示例说明结构层级关系：

```
Minor → Inappropriate Behavior Involving Minors → Delinquent Social Atmosphere → Underage Drinking
```

该结构有利于逐级辨别，增强模型分类与解释能力。([arXiv][1])

虽然完整标签列表未独立开源，但论文提供了示例路径可供借鉴。

---

## 四、模型与训练机制

* **Stage 1**：使用监督微调（SFT）模型，完成安全 vs 风险的初步筛查。([GitHub][2])

* **Stage 2**：基于 MLLM（大模型）使用带 GRPO 的强化训练执行层级分类，并生成理由。([arXiv][1])

* **效果指标**：

  * 整体风险分类准确率提升至 **85.06%**。
  * 精度约 **51.09%**，召回率约 **79.14%**。
  * 部署中人工复核比例减至 **0.24%**，人力审核率降低 **56.38%**。([arXiv][1])

---

## 五、开源情况

* **模型代码**：完整开源，遵循 **Apache-2.0** 许可证，可在 GitHub 获取与直接使用。([GitHub][2])

* **数据集**：训练所用的真实审核样本 **未开源**，涉及隐私与合规敏感，因此仅内部使用。

* **标签体系**：虽属架构核心，但完整 taxonomy 未作为文件开源，仅在论文中示例展现，需自行提取或构建。

---


# **ICM-Assistant**

---

## 一、核心方法（Methodology）

* **规则驱动的数据生成流程**
  将简明的人工审核规则拆解成生成提示（prompts）层级，通过多阶段模板和图像注释，制作一个结构化的 **ICM-Instruct 数据集**，具备逻辑说明和问答形式。([arXiv][1], [ResearchGate][2])

* **Instruction-tuning 多模态大模型**
  选用多个 MLLMs（如 LLaVA v1.5/1.6、mPLUG-Owl-2、Qwen-VL），进行 instruction-tuning。模型不仅能做分类，还能生成解释与问答，紧密遵循规则指令。

* **任务融合：分类 + 解释 + QA**
  调整模型方式，使其在审核分类的基础上输出解释性文本和问答桥接流程，提升可解释性与审核一致性。([ResearchGate][2])

---

## 二、实验数据集（Datasets）

* **ICM-Instruct 数据集**

  * 包含详细的**分类解释与 Q-A 对**用于 supervision。
  * **Validation 集公开**，包含 1,623 张图片、35 个分类；其中有一个子集 `val_MEQ`（379 张）用于评价“Moderation Explanation Quality”指标。
  * **训练集尚未开源**，计划未来发布。

---

## 三、标签分类（Taxonomy）

* **规则驱动分类路径**
  标签项基于平台规则拆解，并用提示逐步生成。具体 label 数量与完整分类结构尚未公开，但 validation set 涵盖 35 类。([Hugging Face][3], [ResearchGate][2])

* **内容结构丰富**
  包括分类判断、解释推理与 QA 三个组成部分，每条标签都关联对应的规则完整性说明。

---

## 四、模型与效果（Models & Performance）

* **多模型支持**
  包括 LLaVA-v1.5/1.6、mPLUG-Owl-2、Qwen-VL 等多个视觉语言模型，通过 instruction-tuning 适配审核任务。([GitHub][4], [Hugging Face][5])

* **效果提升显著**

  * 审核分类性能提升约 **36.8%**
  * 审核解释质量提升约 **26.6%**（相较于未经 tuning 的 MLLMs）([arXiv][1], [ResearchGate][2])

* **Benchmarks**
  提供多任务评估基准，包括分类、解释质量（MEQ），支持图像审核标准测试集和定制规则场景测试。

---

## 五、开源情况（Open Source Status）

* **代码与模型权重（部分）开源**
  GitHub 仓库提供训练脚本、模型调优代码，以及部分已 fine-tuned 权重；研究使用者可免费使用，商业用途需邮件申请权限。([GitHub][4])

* **Validation 数据集公开**
  包括 validation 和 MEQ 子集；训练集和更多数据尚未发布。([Hugging Face][3])

* **使用许可**
  研究用途开放，商业用途需要获得团队书面许可。([GitHub][4])

---


