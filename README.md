
| 库名称                          | 主要检测内容                  | 是否使用 AI 模型                | 模型开源性           | 使用语言 / 技术栈                | 部署/使用方式                  | 典型特性                               |
| ---------------------------- | ----------------------- | ------------------------- | --------------- | ------------------------- | ------------------------ | ---------------------------------- |
| **nsfw-detection (amshrbo)** | 裸体（NudeNet）、暴力、毒品、自然场景  | 是（NudeNet + MobileNet 微调） | 模型开源，但训练数据有限    | Python、Keras、Flask        | 提供 Flask API，支持批量图像检测    | 准确率 >90%，支持多类别，API 化，GPL-3.0       |
| **SafeVision**               | 裸体（图像、视频、直播、摄像头）        | 是（ONNX 深度学习模型）            | 模型随仓库提供，可本地运行   | Python、ONNX Runtime       | CLI、GUI、REST API、实时流媒体处理 | 功能全面，支持桌面 GUI，隐私友好，本地推理，Apache-2.0 |
| **ifnude**                   | 裸体部位（胸部、臀部、性器官），支持照片+绘画 | 是（基于 NudeNet 模型）          | 模型随库分发，MIT 协议   | Python                    | pip 包，本地函数调用             | 简洁轻量，直接返回边界框 + 置信度，支持打码            |
| **HaramBlur**                | 网页图像/视频中的裸体、NSFW 内容     | 是（nsfwjs + Human 库）       | 使用开源 NSFWJS 模型  | JavaScript/HTML，浏览器扩展     | Chrome、Firefox、Safari 扩展 | 实时模糊，悬停可解码，隐私保护（本地推理），AGPL-3.0     |
| **NSFW Filter**              | 网页图像中的色情/NSFW 内容        | 是（NSFWJS 模型，TF.js）        | NSFWJS 开源 (MIT) | TypeScript、TensorFlow\.js | 浏览器扩展（Chrome、Firefox）    | 本地运行，不上传数据，自动屏蔽 NSFW 图像，GPL-3.0    |




---

## 项目概览: nsfw-detection

* **名称**：`nsfw-detection` 由用户 *amshrbo* 发布。
* **目标**：检测图像中的 **裸体、暴力、毒品** 等敏感信息，并基于这些内容为图像打分，指示其“是否适合出现在工作环境”。
* **技术栈**：Python、Keras (MobileNet)、Flask API。
* **许可协议**：GPL‑3.0 ([GitHub][1])
* **GitHub Stats**：约 23 顶星、3 次 fork，活跃度中等 ([GitHub][1])
* **项目类型**：包含数据预处理、模型训练、API 服务端点等完整流水线实现结构。

---

## 功能与实现亮点

### 1. **裸体检测**

* 使用 **NudeNet**（来自 notAI）库检测包含**裸体内容**的图像 ([GitHub][1])。

### 2. **暴力与毒品识别**

* 基于 **Keras−MobileNet** 使用迁移学习方法，并对最后十层进行了微调和 L2 正则化，以识别\*\*暴力（violence）**和**毒品（drugs）\*\*场景。模型在自采数据上取得超过 **90% 的测试集准确率**。
* 数据来自 Google、DuckDuckGo、Bing 等搜索引擎批量下载工具抓取约 1000 张每类图像，之后通过 `ImageDataGenerator` 扩充至每类约 3000 张，包括 *natural* 类 (自然场景) 作为对照类 ([GitHub][1])。

### 3. **Flask API 服务**

* 项目最终构建成一个 **Python Flask API**，支持上传一张或多张图像，返回“是否适合工作环境”的评分。([GitHub][1])

### 4. **项目结构**

包含多个模块文件夹：

* `data_preprocessing`：处理图像、展开标注等。
* `model` 与 `training_and_loading`：模型定义、训练脚本。
* `helpers`：数据增强与模型工具的方法集合。
* `app.py`：启动 Flask 服务接口。([GitHub][1])

---

## 核心流程概要

1. **数据准备**

   * 收集原始图片 (\~1000 张/类别)。
   * 使用 Keras `ImageDataGenerator` 进行数据增强，提升样本数量与差异性。([GitHub][1])

2. **模型训练**

   * 载入 MobileNet 预训练模型，冻结除后 10 层之外的所有权重。
   * 应用 L2 正则防止过拟合。
   * 训练多分类任务：nudity、violence、drugs、natural。([GitHub][1])

3. **性能**

   * 在测试集上模型准确率超过 90%，但作者指出样本分布有限，后续版本计划扩大数据集以提升泛化能力。([GitHub][1])

4. **部署形式**

   * 使用 Flask 提供 API，将本地推理结果打包成 “工作适合度” 评分返回。

5. **后续规划**

   * 收集更多暴力数据（如真实视频帧）。
   * 可能会移除“毒品”类别以聚焦精度更优的类别。
   * 考虑升级到更强主干模型（如 ResNet/VGG）。([GitHub][1])

---

## 一览总结表

| 特性         | 描述                              |
| ---------- | ------------------------------- |
| **检测内容**   | 裸体（NudeNet）、暴力、毒品、自然场景对照        |
| **核心方法**   | MobileNet + Keras 迁移学习 + L2 正则化 |
| **数据来源**   | 搜索引擎抓取 + 数据增强                   |
| **模型准确度**  | 测试集准确率超过 90%                    |
| **部署形式**   | Flask REST API，可处理单/多图像请求       |
| **许可证**    | GPL-3.0（强开源、传染性许可证）             |
| **后续增强方向** | 扩大数据、升级模型、优化性能                  |

---

## 使用建议

* **快速试用**：克隆仓库，按照 README 在虚拟环境里运行 `app.py`，即可快速试验 API 推理效果。
* **迁移至视频处理**：可结合 OpenCV 提取视频帧，循环调用该 API，对视频敏感片段进行识别与打码。
* **精度优化**：若项目需要更严格准确性，可替换模型为 ResNet/VGG 并扩充训练集。
* **注意许可**：GPL-3.0 许可证意味着若将该代码整合进其他项目，对外发布时必须开源衍生作品。

---

## 项目概览: SafeVision

* **项目名称**：SafeVision
* **功能**：一站式解决方案，用于检测并模糊图像、视频、直播流或摄像头画面中的裸露内容，实现实时内容审核与自动遮挡功能。([GitHub][1])
* **最新版本**：当前最新发布的是 **v2.0**（于 2025 年 7 月 25 日发布）。([GitHub][2])
* **语言 & 技术栈**：100% 使用 **Python** 编写，采用 **ONNX** 深度学习模型进行推理。([GitHub][1])
* **开源协议**：采用 **Apache-2.0** 协议，宽松且商业友好。([GitHub][1])
* **社区状况**：获得 20 个星标（Stars）、1 次 Fork，显示出一定使用兴趣并仍在维护中。([GitHub][1], [repos.ecosyste.ms][3])

---

## 核心特点与功能模块

| 功能分类        | 描述                                                              |
| ----------- | --------------------------------------------------------------- |
| **检测 + 打码** | 利用 ONNX 模型实现对裸露内容的检测与自动模糊。适用于图像、视频、直播流甚至摄像头画面。([GitHub][1])     |
| **多接口支持**   | 提供多种使用方式，包括 CLI、GUI（图形界面）、实时监视、流媒体处理和 RESTful API。([GitHub][1]) |
| **配置灵活**    | 支持自定义模糊规则，可设定只打码特定部位，或排除某些内容。([GitHub][2])                      |
| **日志记录**    | 自动生成详细日志，记录检测结果便于审计和分析。([GitHub][2])                            |

---

## 使用场景与用户界面

* **CLI（命令行）模式**：可快速处理批量图像或视频，适合自动化脚本集成。
* **GUI（桌面界面）**：拖拽即可查看输入与检测结果的实时预览，适合非技术用户操作。([GitHub][2])
* **实时流 & 摄像头处理**：直接应用于直播场景，适合内容审核或在线监控系统。([GitHub][1])
* **REST API**：可通过 HTTP 接口集成至其他系统、微服务或前端应用，灵活性强。([GitHub][1])

---

## 技术优势与适用价值

* **ONNX 深度学习模型**：支持跨平台部署，推理速度快，适配 GPU/CPU。([GitHub][1])
* **多模式应用能力**：无论是静图、视频、直播或摄像头，都有相应模块支持，适配性优。
* **隐私保护**：所有识别与模糊处理均在本地执行，无需上传数据到服务器，保护用户隐私。
* **可扩展与开源**：Apache-2.0 许可证允许在商业项目中使用、修改，并与公司产品集成。

---

## 快速上手指南

1. **克隆仓库**

   ```bash
   git clone https://github.com/im-syn/SafeVision.git
   cd SafeVision
   ```

2. **安装依赖**

   ```bash
   pip install -r requirements.txt
   ```

3. **下载模型**
   仓库 README 通常提供模型下载链接或说明，确保模型目录结构正确。

4. **运行方式示例**

   * **模糊处理图像**

     ```bash
     python main.py --input input.jpg --output output.jpg
     ```
   * **启动 GUI**

     ```bash
     python SafeVisionGUI.py
     ```
   * **视频或直播模糊**

     ```bash
     python live.py --source webcam
     ```
   * **API 服务启动**

     ```bash
     python safevision_api.py
     ```

5. **日志与配置**

   * 可查看日志输出的检测框、部位位置、置信度等详细信息。
   * 支持配置例外规则（如“仅隐藏特定部位”或“不打码胸部”）进行灵活调整。([GitHub][2])

---

## 总结推荐

**SafeVision** 是一个功能强大、界面友好的 Python 工具，适合多种平台（桌面、API、视频、流媒体）实现裸体检测与自动模糊。其特点包括：

* **全面覆盖多种输入形式**；
* **多端支持（GUI、CLI、API）**；
* **本地推理、高度隐私保护**；
* **部署灵活、开源协议友好**。

---

## 项目概览: ifnude

**ifnude** 是一个由 *s0md3v* 开发的 Python 库，能够准确检测图像中的裸露部位，无论是真人照片还是绘画作品。同时具备对检测结果进行打码（遮挡）的功能。(\[turn0search0])

* **全称**：ifnude — nudity detector that works
* **核心功能**：识别图像中具体裸露部位，并可选地进行遮挡（模糊或遮盖）
* **适用内容**：既支持真人图像，也支持绘画、插画等非写实图形
* **许可协议**：MIT 开源许可证 (\[turn0search0])

---

## 使用安装与示例代码

在 Python 环境中，通过 pip 即可快速安装并使用：

```bash
pip install ifnude
```

（首次导入将自动下载 \~139MB 的模型数据至系统目录，后续无需再次下载）(\[turn0search0])

示例用法：

```python
from ifnude import detect

results = detect('/path/to/nsfw.png')
print(results)
```

**输出示例**：

```json
[
  {
    "box": [164, 188, 246, 271],
    "score": 0.8253,
    "label": "EXPOSED_BREAST_F"
  },
  {
    "box": [252, 190, 335, 270],
    "score": 0.8236,
    "label": "EXPOSED_BREAST_F"
  }
]
```

输出结果为一个数组，每个元素代表检测到的敏感部位，包括边界框位置、置信度、以及部位类别标签。(\[turn0search0], \[turn0search2])

---

## 模型来源与性质

* **模型来源**：该项目是对原始 **NudeNet** 库的“Fork”版本。作者删除了原有的“视频检测”功能，因其在实践中易发生崩溃；但未来计划重新实现视频支持功能。(\[turn0search0], \[turn0search2])
* **模型类型**：依赖 neural network 模型，精准识别裸露内容。
* **是否 AI 驱动**：是，基于神经网络训练的模型。
* **模型开源**：模型本身随仓库公开，但具体训练细节或训练代码并未明确公开。

---

## 语言与运行环境

* **编程语言**：100% 使用 **Python** 开发。(\[turn0search0])
* **Python 版本兼容性**：兼容 Python 3.6 及以上版本，也可在 CPython 和 PyPy 上运行。(\[turn0search2])
* **运行方式**：在本地环境运行，无需依赖云服务或外部 API。

---

## 基本特征一览表

| 属性        | 说明                   |
| --------- | -------------------- |
| **功能**    | 检测并可选地对裸露部位进行打码      |
| **AI 模型** | 使用基于 neural nets 的模型 |
| **模型开源性** | 包含在项目中，但训练过程与数据未详述   |
| **语言**    | Python               |
| **使用方式**  | pip 安装 + 本地推理运行      |
| **适用内容**  | 真人图像与绘画作品            |
| **许可协议**  | MIT（开源自由）            |

---

## 小结与建议

**ifnude** 是一个轻量、易上手的 Nude 检测工具，适合快速集成到图像处理、隐私保护或审核流程中。其优势在于：

* 使用简单、无需构建复杂模型；
* 支持多种图像类型；
* 本地运行保护隐私；
* 开源度高，可按照 MIT 协议自由使用和修改。

但也要注意：

* 暂不支持视频处理功能；
* 若有更高精度或自定义需求，可考虑训练模型或使用其他更先进框架（如 NudeNet v3、基于 ONNX 的方案等）。


---

## 项目概览: HaramBlur

* **名称**：HaramBlur
* **类型**：开源浏览器扩展，旨在帮助用户在网页上屏蔽违反伊斯兰教义的敏感内容，保护“视线清净”原则。([GitHub][1])
* **授权协议**：AGPL‑3.0 开源许可，鼓励社区贡献与分享。([GitHub][1])
* **代码语言构成**：JavaScript（约占 70.2%），HTML（约占 29.8%）([github-zh.com][2])

---

## 核心功能与工作机制

* **模糊目标**：对网页中的图片与视频进行实时检测与模糊处理，支持用户设置模糊类型与严格程度，具备“鼠标悬停解除模糊”功能。([GitHub][1])
* **双重 AI 检测**：

  * 使用 *Human* 库进行人脸检测。
  * 使用 *nsfwjs*（基于 TensorFlow\.js）识别 NSFW 内容（例如裸体、色情）。([github-zh.com][2], [GitHub][1])
* **本地识别**：所有内容识别与模糊操作均在用户浏览器本地进行，不上传任何图像或浏览数据，强调隐私。([东艾国际][3])
* **可配置性**：用户可调整模糊强度、检测严格度，以及选择模糊类型（图片、视频）。支持开关控制和悬停解除模糊。([GitHub][1])

---

## 支持平台与安装方式

* **可用平台**：

  * **Chrome / Chromium 系浏览器**（包括 Edge、Brave），目前已有约 40,000 名活跃用户，评分约 4.9/5。([HaramBlur][4])
  * **Firefox / Android 版 Firefox**：评分约 4.4，可安装于桌面及移动上。([addons.mozilla.org][5])
  * **Safari（macOS）**：目前已推出。iOS 及 Android 应用正在计划中。([HaramBlur][4])
* **安装方式**：

  * 直接从官方扩展商店安装（Chrome Web Store、Firefox Add-ons）。
  * 或从 GitHub 克隆源码，本地构建后手动加载（使用 `npm install` + `npm run build` + `Load unpacked`）。([GitHub][1])

---

## 技术挑战 & 性能考量

* **Manifest V3 限制**：由于 Manifest V3 不支持持久背景脚本，因此每个标签页刷新时都得重新初始化模型，导致性能下降。开发者正在探讨使用 Offscreen API 或其他方式潇洒解决该问题。([GitHub][6])

---

## 社区反响与使用体验

* **用户评价普遍正面**：

  * “Fantastic work…”
  * “May Allah reward you immensely” 等评论反映出产品在精神支持层面的价值。([HaramBlur][4])
* **常见反馈问题**：

  * **误伤（False Positives）**：偶尔会对一些无害物体（如门、果实、缩略图）进行模糊。
  * **漏检（Missed Detections）**：部分敏感内容未被模糊处理。
  * **模糊功能失效**：个别情况下，模糊效果自动失效或没有生效。
  * **平台覆盖有限**：移动端（Android / iOS）版本尚在开发中。([Chrome-Stats][7])

---

## 总结一览

| 类别   | 说明                                       |
| ---- | ---------------------------------------- |
| 功能目标 | 浏览器中自动检测并模糊不符合伊斯兰教义的敏感内容（图片、视频）          |
| 模型机制 | 使用 Human（人脸检测） + nsfwjs（NSFW 内容检测）进行混合判断 |
| 隐私保护 | 全部识别在用户本地浏览器处理，不追踪或上传用户内容                |
| 可配置性 | 支持模糊强度、检测严格度、开启/关闭、悬停解除模糊等配置             |
| 性能挑战 | Manifest V3 的限制导致性能下降，待优化中               |
| 社区反馈 | 口碑良好，但存在误伤、漏检等问题，移动版仍在规划中                |

---

---

## 项目概览：NSFW Filter

* **全称**：`nsfw-filter/nsfw-filter`
* **简介**：一个 **免费、开源、注重隐私** 的浏览器扩展，致力于屏蔽 Not Safe For Work（NSFW）的不良内容。([GitHub][1])
* **核心技术**：

  * 使用 **TypeScript** 编写
  * 基于 **TensorFlow\.js** 与 **NSFWJS** 实施模型推理([GitHub][1])
* **许可协议**：采用 **GPL‑3.0** 开源协议([GitHub][2])
* **上次更新**：最近一次提交日期为 **2024 年 8 月 7 日**([npm][3])

---

## 是否使用 AI 模型？模型是否开源？

* **使用了 AI 模型**：该扩展集成了 **NSFWJS**，这是一个基于 TensorFlow\.js 的模型库，用于客户端图像 NSFW 检测。([npm][4])
* **模型开源性**：NSFWJS 是 **MIT 许可** 的开源项目，开发者可以自行下载并托管模型文件供加载。([npm][4])
* **模型训练背景**：NSFWJS 的模型是在大量 NSFW 图像数据上，使用 **Keras 训练**完成的，模型结构和权重在其 GitHub 项目中公开。([npm][5])

---

## 使用语言与技术栈

* **编程语言**：主要使用 **TypeScript**，适合开发现代浏览器扩展与搭配静态类型保障。([GitHub][1])
* **模型平台**：在客户端使用 **TensorFlow\.js** 执行 NSFW 检测模型；支持浏览器环境（WebGL/WASM）与 Node.js。([GitHub][1])
* **扩展平台**：专为浏览器扩展（如 Chrome、Firefox）开发，也可以嵌入前端 Web 应用或 Node.js 环境中。([GitHub][2])

---

## 使用流程（原理简述）

1. **加载模型**
   使用 NSFWJS 提供的 `load()` 方法，在客户端加载预训练模型（可托管在自有服务器）。([npm][4])

2. **图像分类**
   调用模型的 `classify(image)` 方法，对图像进行分类，输出多种类别的概率分布，如：

   * `Drawing`, `Hentai`, `Neutral`, `Porn`, `Sexy`([npm][4])

3. **判断与处理**
   扩展读取分类结果，若分类属于不合规（如 `Porn`、`Hentai`），则在页面上自动遮盖或模糊相关图像。

4. **全部在客户端运行**
   为保护用户隐私，所有识别与判断逻辑均在用户浏览器本地执行，不会上传图像数据到服务器。

---

## 小结对照表

| 项目属性          | 说明                                  |
| ------------- | ----------------------------------- |
| **功能目的**      | 浏览器中自动检测并屏蔽 NSFW 图像内容               |
| **是否用 AI 模型** | 是，使用 NSFWJS（TF.js 模型）               |
| **模型开源吗？**    | 是，MIT 许可证开源                         |
| **构建语言**      | TypeScript                          |
| **运行平台**      | 浏览器扩展（Chrome、Firefox），可集成前端/Node.js |
| **隐私特点**      | 模型本地运行，不上传图片数据                      |

---




